7767517
81 80
pnnx.Input               pnnx_input_0             0 1 0 #0=(1,3,32,32)f32
pnnx.Input               pnnx_input_1             0 1 1 #1=(1)i64
pnnx.Attribute           pnnx_fold_div_term.1     0 1 2 @data=(15)f32 #2=(15)f32
pnnx.Attribute           pnnx_fold_div_term.1_1   0 1 3 @data=(15)f32 #3=(15)f32
torch.unsqueeze          torch.unsqueeze_16       1 1 1 4 dim=1 $input=1 #1=(1)i64 #4=(1,1)i64
Tensor.to                Tensor.to_11             1 1 4 5 copy=False dtype=torch.float $input=4 #4=(1,1)i64 #5=(1,1)f32
pnnx.Expression          pnnx_expr_76             2 1 5 2 6 expr=sin(mul(@0,@1)) #5=(1,1)f32 #2=(15)f32 #6=(1,15)f32
pnnx.Expression          pnnx_expr_74             2 1 5 3 7 expr=cos(mul(@0,@1)) #5=(1,1)f32 #3=(15)f32 #7=(1,15)f32
torch.stack              torch.stack_15           2 1 6 7 8 dim=-1 #6=(1,15)f32 #7=(1,15)f32 #8=(1,15,2)f32
Tensor.reshape           Tensor.reshape_9         1 1 8 9 shape=(1,30) $input=8 #8=(1,15,2)f32 #9=(1,30)f32
nn.Conv2d                image_to                 1 1 0 10 bias=True dilation=(1,1) groups=1 in_channels=3 kernel_size=(3,3) out_channels=64 padding=same padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,3,3,3)f32 #0=(1,3,32,32)f32 #10=(1,64,32,32)f32
nn.Linear                enc.0.temb_to_ch         1 1 9 11 bias=True in_features=30 out_features=64 @bias=(64)f32 @weight=(64,30)f32 #9=(1,30)f32 #11=(1,64)f32
nn.Conv2d                enc.0.conv1              1 1 10 12 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=same padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 #10=(1,64,32,32)f32 #12=(1,64,32,32)f32
nn.ReLU                  enc.0.act                1 1 12 13 #12=(1,64,32,32)f32 #13=(1,64,32,32)f32
Tensor.reshape           torch.unsqueeze_18       1 1 11 14 shape=(1,64,1,1) $input=11 #11=(1,64)f32 #14=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_63             2 1 13 14 15 expr=add(@0,@1) #13=(1,64,32,32)f32 #14=(1,64,1,1)f32 #15=(1,64,32,32)f32
nn.Conv2d                enc.0.conv2              1 1 15 16 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=same padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 #15=(1,64,32,32)f32 #16=(1,64,32,32)f32
nn.ReLU                  pnnx_unique_0            1 1 16 17 #16=(1,64,32,32)f32 #17=(1,64,32,32)f32
nn.Conv2d                enc.0.conv3              1 1 17 18 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=same padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 #17=(1,64,32,32)f32 #18=(1,64,32,32)f32
nn.ReLU                  pnnx_unique_1            1 1 18 19 #18=(1,64,32,32)f32 #19=(1,64,32,32)f32
pnnx.Expression          pnnx_expr_61             2 1 19 10 20 expr=add(@0,@1) #19=(1,64,32,32)f32 #10=(1,64,32,32)f32 #20=(1,64,32,32)f32
torch.clone              torch.clone_5            1 1 20 21 $input=20 #20=(1,64,32,32)f32 #21=(1,64,32,32)f32
nn.Conv2d                downsamplers.0           1 1 20 22 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(2,2) out_channels=128 padding=(0,0) padding_mode=zeros stride=(2,2) @bias=(128)f32 @weight=(128,64,2,2)f32 #20=(1,64,32,32)f32 #22=(1,128,16,16)f32
nn.Linear                enc.1.temb_to_ch         1 1 9 23 bias=True in_features=30 out_features=128 @bias=(128)f32 @weight=(128,30)f32 #9=(1,30)f32 #23=(1,128)f32
nn.Conv2d                enc.1.conv1              1 1 22 24 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=same padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,3,3)f32 #22=(1,128,16,16)f32 #24=(1,128,16,16)f32
nn.ReLU                  enc.1.act                1 1 24 25 #24=(1,128,16,16)f32 #25=(1,128,16,16)f32
Tensor.reshape           torch.unsqueeze_20       1 1 23 26 shape=(1,128,1,1) $input=23 #23=(1,128)f32 #26=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_51             2 1 25 26 27 expr=add(@0,@1) #25=(1,128,16,16)f32 #26=(1,128,1,1)f32 #27=(1,128,16,16)f32
nn.Conv2d                enc.1.conv2              1 1 27 28 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=same padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,3,3)f32 #27=(1,128,16,16)f32 #28=(1,128,16,16)f32
nn.ReLU                  pnnx_unique_2            1 1 28 29 #28=(1,128,16,16)f32 #29=(1,128,16,16)f32
nn.Conv2d                enc.1.conv3              1 1 29 30 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=same padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,3,3)f32 #29=(1,128,16,16)f32 #30=(1,128,16,16)f32
nn.ReLU                  pnnx_unique_3            1 1 30 31 #30=(1,128,16,16)f32 #31=(1,128,16,16)f32
pnnx.Expression          pnnx_expr_49             2 1 31 22 32 expr=add(@0,@1) #31=(1,128,16,16)f32 #22=(1,128,16,16)f32 #32=(1,128,16,16)f32
torch.clone              torch.clone_6            1 1 32 33 $input=32 #32=(1,128,16,16)f32 #33=(1,128,16,16)f32
nn.Conv2d                downsamplers.1           1 1 32 34 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(2,2) out_channels=256 padding=(0,0) padding_mode=zeros stride=(2,2) @bias=(256)f32 @weight=(256,128,2,2)f32 #32=(1,128,16,16)f32 #34=(1,256,8,8)f32
nn.Linear                enc.2.temb_to_ch         1 1 9 35 bias=True in_features=30 out_features=256 @bias=(256)f32 @weight=(256,30)f32 #9=(1,30)f32 #35=(1,256)f32
nn.Conv2d                enc.2.conv1              1 1 34 36 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=same padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #34=(1,256,8,8)f32 #36=(1,256,8,8)f32
nn.ReLU                  enc.2.act                1 1 36 37 #36=(1,256,8,8)f32 #37=(1,256,8,8)f32
Tensor.reshape           torch.unsqueeze_22       1 1 35 38 shape=(1,256,1,1) $input=35 #35=(1,256)f32 #38=(1,256,1,1)f32
pnnx.Expression          pnnx_expr_39             2 1 37 38 39 expr=add(@0,@1) #37=(1,256,8,8)f32 #38=(1,256,1,1)f32 #39=(1,256,8,8)f32
nn.Conv2d                enc.2.conv2              1 1 39 40 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=same padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #39=(1,256,8,8)f32 #40=(1,256,8,8)f32
nn.ReLU                  pnnx_unique_4            1 1 40 41 #40=(1,256,8,8)f32 #41=(1,256,8,8)f32
nn.Conv2d                enc.2.conv3              1 1 41 42 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=256 padding=same padding_mode=zeros stride=(1,1) @bias=(256)f32 @weight=(256,256,3,3)f32 #41=(1,256,8,8)f32 #42=(1,256,8,8)f32
nn.ReLU                  pnnx_unique_5            1 1 42 43 #42=(1,256,8,8)f32 #43=(1,256,8,8)f32
pnnx.Expression          pnnx_expr_37             2 1 43 34 44 expr=add(@0,@1) #43=(1,256,8,8)f32 #34=(1,256,8,8)f32 #44=(1,256,8,8)f32
torch.flatten            torch.flatten_14         1 1 44 45 end_dim=-1 start_dim=2 $input=44 #44=(1,256,8,8)f32 #45=(1,256,64)f32
Tensor.permute           Tensor.permute_7         1 1 45 46 dims=(0,2,1) $input=45 #45=(1,256,64)f32 #46=(1,64,256)f32
nn.Linear                to_query                 1 1 46 47 bias=True in_features=256 out_features=256 @bias=(256)f32 @weight=(256,256)f32 #46=(1,64,256)f32 #47=(1,64,256)f32
nn.Linear                to_key                   1 1 46 48 bias=True in_features=256 out_features=256 @bias=(256)f32 @weight=(256,256)f32 #46=(1,64,256)f32 #48=(1,64,256)f32
nn.Linear                to_value                 1 1 46 49 bias=True in_features=256 out_features=256 @bias=(256)f32 @weight=(256,256)f32 #46=(1,64,256)f32 #49=(1,64,256)f32
nn.MultiheadAttention    attention                3 1 47 48 49 50 add_bias_kv=False add_zero_attn=False batch_first=True bias=True embed_dim=256 kdim=256 num_heads=1 vdim=256 @in_proj_bias=(768)f32 @in_proj_weight=(768,256)f32 @out_proj.bias=(256)f32 @out_proj.weight=(256,256)f32 #47=(1,64,256)f32 #48=(1,64,256)f32 #49=(1,64,256)f32 #50=(1,64,256)f32
nn.Linear                out_proj                 1 1 50 51 bias=True in_features=256 out_features=256 @bias=(256)f32 @weight=(256,256)f32 #50=(1,64,256)f32 #51=(1,64,256)f32
Tensor.permute           Tensor.permute_8         1 1 51 52 dims=(0,2,1) $input=51 #51=(1,64,256)f32 #52=(1,256,64)f32
Tensor.reshape           Tensor.reshape_10        1 1 52 53 shape=(1,256,8,8) $input=52 #52=(1,256,64)f32 #53=(1,256,8,8)f32
pnnx.Expression          pnnx_expr_26             2 1 44 53 54 expr=add(@0,@1) #44=(1,256,8,8)f32 #53=(1,256,8,8)f32 #54=(1,256,8,8)f32
nn.ConvTranspose2d       upsamplers.0             1 1 54 55 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(2,2) out_channels=128 output_padding=(0,0) padding=(0,0) stride=(2,2) @bias=(128)f32 @weight=(256,128,2,2)f32 #54=(1,256,8,8)f32 #55=(1,128,16,16)f32
torch.cat                torch.cat_12             2 1 55 33 56 dim=1 #55=(1,128,16,16)f32 #33=(1,128,16,16)f32 #56=(1,256,16,16)f32
nn.Linear                dec.0.temb_to_ch         1 1 9 57 bias=True in_features=30 out_features=128 @bias=(128)f32 @weight=(128,30)f32 #9=(1,30)f32 #57=(1,128)f32
nn.Conv2d                dec.0.conv1              1 1 56 58 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(3,3) out_channels=128 padding=same padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,256,3,3)f32 #56=(1,256,16,16)f32 #58=(1,128,16,16)f32
nn.ReLU                  dec.0.act                1 1 58 59 #58=(1,128,16,16)f32 #59=(1,128,16,16)f32
Tensor.reshape           torch.unsqueeze_24       1 1 57 60 shape=(1,128,1,1) $input=57 #57=(1,128)f32 #60=(1,128,1,1)f32
pnnx.Expression          pnnx_expr_15             2 1 59 60 61 expr=add(@0,@1) #59=(1,128,16,16)f32 #60=(1,128,1,1)f32 #61=(1,128,16,16)f32
nn.Conv2d                dec.0.conv2              1 1 61 62 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=same padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,3,3)f32 #61=(1,128,16,16)f32 #62=(1,128,16,16)f32
nn.ReLU                  pnnx_unique_6            1 1 62 63 #62=(1,128,16,16)f32 #63=(1,128,16,16)f32
nn.Conv2d                dec.0.conv3              1 1 63 64 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=128 padding=same padding_mode=zeros stride=(1,1) @bias=(128)f32 @weight=(128,128,3,3)f32 #63=(1,128,16,16)f32 #64=(1,128,16,16)f32
nn.ReLU                  pnnx_unique_7            1 1 64 65 #64=(1,128,16,16)f32 #65=(1,128,16,16)f32
pnnx.Expression          pnnx_expr_13             2 1 65 55 66 expr=add(@0,@1) #65=(1,128,16,16)f32 #55=(1,128,16,16)f32 #66=(1,128,16,16)f32
nn.ConvTranspose2d       upsamplers.1             1 1 66 67 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(2,2) out_channels=64 output_padding=(0,0) padding=(0,0) stride=(2,2) @bias=(64)f32 @weight=(128,64,2,2)f32 #66=(1,128,16,16)f32 #67=(1,64,32,32)f32
torch.cat                torch.cat_13             2 1 67 21 68 dim=1 #67=(1,64,32,32)f32 #21=(1,64,32,32)f32 #68=(1,128,32,32)f32
nn.Linear                dec.1.temb_to_ch         1 1 9 69 bias=True in_features=30 out_features=64 @bias=(64)f32 @weight=(64,30)f32 #9=(1,30)f32 #69=(1,64)f32
nn.Conv2d                dec.1.conv1              1 1 68 70 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(3,3) out_channels=64 padding=same padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,128,3,3)f32 #68=(1,128,32,32)f32 #70=(1,64,32,32)f32
nn.ReLU                  dec.1.act                1 1 70 71 #70=(1,64,32,32)f32 #71=(1,64,32,32)f32
Tensor.reshape           torch.unsqueeze_26       1 1 69 72 shape=(1,64,1,1) $input=69 #69=(1,64)f32 #72=(1,64,1,1)f32
pnnx.Expression          pnnx_expr_2              2 1 71 72 73 expr=add(@0,@1) #71=(1,64,32,32)f32 #72=(1,64,1,1)f32 #73=(1,64,32,32)f32
nn.Conv2d                dec.1.conv2              1 1 73 74 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=same padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 #73=(1,64,32,32)f32 #74=(1,64,32,32)f32
nn.ReLU                  pnnx_unique_8            1 1 74 75 #74=(1,64,32,32)f32 #75=(1,64,32,32)f32
nn.Conv2d                dec.1.conv3              1 1 75 76 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=64 padding=same padding_mode=zeros stride=(1,1) @bias=(64)f32 @weight=(64,64,3,3)f32 #75=(1,64,32,32)f32 #76=(1,64,32,32)f32
nn.ReLU                  pnnx_unique_9            1 1 76 77 #76=(1,64,32,32)f32 #77=(1,64,32,32)f32
pnnx.Expression          pnnx_expr_0              2 1 77 67 78 expr=add(@0,@1) #77=(1,64,32,32)f32 #67=(1,64,32,32)f32 #78=(1,64,32,32)f32
nn.Conv2d                to_image                 1 1 78 79 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=3 padding=same padding_mode=zeros stride=(1,1) @bias=(3)f32 @weight=(3,64,1,1)f32 #78=(1,64,32,32)f32 #79=(1,3,32,32)f32
pnnx.Output              pnnx_output_0            1 0 79 #79=(1,3,32,32)f32
